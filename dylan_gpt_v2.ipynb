{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "collapsed_sections": [
        "oXXdyMtpOXRE",
        "cllahlv10aq4",
        "UesWYZfDb7rK"
      ],
      "authorship_tag": "ABX9TyO5hJZB7clSiWFFkDgkjNOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafasacaan/dylan-gpt/blob/main/dylan_gpt_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks7tiVrUbt6m",
        "outputId": "1c468eb0-ee66-4de7-b16e-aff393153d81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "Z3SqoupBncZm"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, using CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIaRgReAPFPr",
        "outputId": "6c7f34fd-7b29-4d48-a65d-3019e7e54a78"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"{device=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcKDH3B5PB3W",
        "outputId": "e7f0527b-3a80-47f7-a584-11ecd367f309"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device=device(type='cuda')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Fi2H4eOF8I",
        "outputId": "8d1fc223-a603-4f6f-8c4a-e4021e9711e8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c1d1584f3f0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Names data"
      ],
      "metadata": {
        "id": "oXXdyMtpOXRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data found in kaggle [here](https://www.kaggle.com/datasets/cloudy17/bob-dylan-songs)"
      ],
      "metadata": {
        "id": "Ry8ha8_kO28m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "KgNVgaTyQeGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c39f078-d777-4716-91b4-aa1889fd62c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-09 14:20:59--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-08-09 14:20:59 (57.6 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "l4pYlm7aQkN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1GBdSamQpyy",
        "outputId": "748cc028-d8bf-4547-d70c-216c781fe917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z48ZlWZaQmtH",
        "outputId": "7317ddc3-3e76-4f49-e465-8772c3543a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mapping from characters to integers and vice versa\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "0n39uJuUQbAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "V4H0AjeCQa9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLH_cdFfQtrE",
        "outputId": "207054de-c8bf-4491-d7b5-879cd3ad5753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dylan Lyrics Data - Chars\n",
        "Load data from [here](https://www.kaggle.com/datasets/cloudy17/bob-dylan-songs)"
      ],
      "metadata": {
        "id": "7IgaCOKjqavC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('clear.csv')"
      ],
      "metadata": {
        "id": "XxGrsUy2qajS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first lyric\n",
        "text = df.lyrics[0]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "61X6z73gqhXa",
        "outputId": "e829e708-e1c8-4cce-ea12-7df09c5660b2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Come you ladies and you gentlemen, a-listen to my song\\n\\nSing it to you right, but you might think it’s wrong\\n\\nJust a little glimpse of a story I’ll tell\\n\\n’Bout an East Coast city that you all know well\\n\\nIt’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nOld New York City is a friendly old town\\n\\nFrom Washington Heights to Harlem on down\\n\\nThere’s a-mighty many people all millin’ all around\\n\\nThey’ll kick you when you’re up and knock you when you’re down\\n\\nIt’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nIt’s a mighty long ways from the Golden Gate\\n\\nTo Rockefeller Plaza ’n’ the Empire State.\\n\\nMister Rockefeller sets up as high as a bird\\n\\nOld Mister Empire never says a word\\n\\nIt’s hard times from the country\\n\\nLivin’ down in New York town\\n\\n\\n\\nWell, it’s up in the mornin’ tryin’ to find a job of work\\n\\nStand in one place till your feet begin to hurt\\n\\nIf you got a lot o’ money you can make yourself merry\\n\\nIf you only got a nickel, it’s the Staten Island Ferry\\n\\nAnd it’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nMister Hudson come a-sailin’ down the stream\\n\\nAnd old Mister Minuet paid for his dream\\n\\nBought your city on a one-way track\\n\\n’F I had my way I’d sell it right back\\n\\nAnd it’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nI’ll take all the smog in Cal-i-for-ne-ay\\n\\n’N’ every bit of dust in the Oklahoma plains\\n\\n’N’ the dirt in the caves of the Rocky Mountain mines\\n\\nIt’s all much cleaner than the New York kind\\n\\nAnd it’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nSo all you newsy people, spread the news around\\n\\nYou c’n listen to m’ story, listen to m’ song\\n\\nYou c’n step on my name, you c’n try ’n’ get me beat\\n\\nWhen I leave New York, I’ll be standin’ on my feet\\n\\nAnd it’s hard times in the city\\n\\nLivin’ down in New York town'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate lyrics into one string\n",
        "text = (\n",
        "  ''.join(\n",
        "    df['lyrics']\n",
        "    .str.replace('\\n \\n \\n \\n ', '\\n\\n\\n')\n",
        "    .str.replace('\\n \\n \\n ', '\\n\\n\\n')\n",
        "    .str.replace('\\n \\n ', '\\n\\n')\n",
        "    .str.replace('\\n ', '\\n')\n",
        "    .str.replace('\\n\\n\\n', '\\n\\n')\n",
        "    .str.replace('\\n\\n', '\\n')\n",
        "  )\n",
        ")"
      ],
      "metadata": {
        "id": "avFZleWCsE1K"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get char counts\n",
        "char_count = {}\n",
        "\n",
        "for char in text:\n",
        "    if char in char_count:\n",
        "        char_count[char] += 1\n",
        "    else:\n",
        "        char_count[char] = 1"
      ],
      "metadata": {
        "id": "4PI2F2Szw5B5"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_char_count =  dict(sorted(char_count.items(), key=lambda item: item[1], reverse=True))"
      ],
      "metadata": {
        "id": "J6A5oIp1xA2c"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "Ysx6nbP2qlhN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEKjrb5Aqaei",
        "outputId": "4b70d964-7569-4733-8fe9-a68aac86d5f3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mapping from characters to integers and vice versa\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "0mdL3cWcx0YI"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "#data = torch.tensor(encode(text), dtype=torch.long)\n",
        "data = np.array(encode(text), dtype=np.int16)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "7kycSr_Px0Vq"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2LN6epIx0TM",
        "outputId": "715248eb-1ce2-4964-dca5-e0ae730251ab"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([26, 65, 63, 55,  2, 75, 65, 71,  2, 62, 51, 54, 59, 55, 69,  2, 51,\n",
              "       64, 54,  2], dtype=int16)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(train_data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GgWwtpz14JPe",
        "outputId": "4baf408d-b534-4da7-ddf6-82eaade67e4b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Come you ladies and you gentlemen, a-listen to my song\\nSing it to you right, but you might think it’'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(''.join(stoi.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52rFwMgf4ZRw",
        "outputId": "19887a46-73fd-4af8-8d81-bb27acdeab5d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u000b !\"'(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ`abcdefghijklmnopqrstuvwxyz¥©éñóü–—‘’“”… \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dylan Lyrics Data - Words"
      ],
      "metadata": {
        "id": "cllahlv10aq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('clear.csv')\n",
        "\n",
        "text = (\n",
        "  ''.join(\n",
        "    df['lyrics']\n",
        "    .str.replace('\\n \\n \\n \\n ', '\\n\\n\\n')\n",
        "    .str.replace('\\n \\n \\n ', '\\n\\n\\n')\n",
        "    .str.replace('\\n \\n ', '\\n\\n')\n",
        "    .str.replace('\\n ', '\\n')\n",
        "    .str.replace('\\n\\n\\n', '\\n\\n')\n",
        "    .str.replace('\\n\\n', '\\n')\n",
        "  )\n",
        ")"
      ],
      "metadata": {
        "id": "Jr5Hfgvr0cKa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "metadata": {
        "id": "2zTF_uRM0kpy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokenized = tokenizer(text)"
      ],
      "metadata": {
        "id": "QVSNRjQp04sH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sorted(list(set(text_tokenized)))\n",
        "vocab_size = len(tokens)\n",
        "\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LqlLY6Z76fk",
        "outputId": "40b6a368-0835-4bd7-d44a-b107a899eead"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8774"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mapping from characters to integers and vice versa\n",
        "stoi = { ch:i for i,ch in enumerate(tokens) }\n",
        "itos = { i:ch for i,ch in enumerate(tokens) }\n",
        "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "CdJL2-Ik04c9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text_tokenized), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "KWbTrOzQ0adk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImJaUg9e8RvO",
        "outputId": "353c2d66-e5cf-4ee5-e53b-23ce1cffa844"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1469, 8498, 4121,  298, 8498, 3089,    4,   70, 7620, 4910])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dylan Lyrics - Words (Karpathy)"
      ],
      "metadata": {
        "id": "UesWYZfDb7rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('clear.csv')"
      ],
      "metadata": {
        "id": "TYeiB2H1b8fW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first lyric\n",
        "text = df.lyrics[0]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "hH_K5NaXcOnp",
        "outputId": "c1cbf713-33a8-4b2d-9503-cbcbf9b46726"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Come you ladies and you gentlemen, a-listen to my song\\n\\nSing it to you right, but you might think it’s wrong\\n\\nJust a little glimpse of a story I’ll tell\\n\\n’Bout an East Coast city that you all know well\\n\\nIt’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nOld New York City is a friendly old town\\n\\nFrom Washington Heights to Harlem on down\\n\\nThere’s a-mighty many people all millin’ all around\\n\\nThey’ll kick you when you’re up and knock you when you’re down\\n\\nIt’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nIt’s a mighty long ways from the Golden Gate\\n\\nTo Rockefeller Plaza ’n’ the Empire State.\\n\\nMister Rockefeller sets up as high as a bird\\n\\nOld Mister Empire never says a word\\n\\nIt’s hard times from the country\\n\\nLivin’ down in New York town\\n\\n\\n\\nWell, it’s up in the mornin’ tryin’ to find a job of work\\n\\nStand in one place till your feet begin to hurt\\n\\nIf you got a lot o’ money you can make yourself merry\\n\\nIf you only got a nickel, it’s the Staten Island Ferry\\n\\nAnd it’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nMister Hudson come a-sailin’ down the stream\\n\\nAnd old Mister Minuet paid for his dream\\n\\nBought your city on a one-way track\\n\\n’F I had my way I’d sell it right back\\n\\nAnd it’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nI’ll take all the smog in Cal-i-for-ne-ay\\n\\n’N’ every bit of dust in the Oklahoma plains\\n\\n’N’ the dirt in the caves of the Rocky Mountain mines\\n\\nIt’s all much cleaner than the New York kind\\n\\nAnd it’s hard times in the city\\n\\nLivin’ down in New York town\\n\\n\\n\\nSo all you newsy people, spread the news around\\n\\nYou c’n listen to m’ story, listen to m’ song\\n\\nYou c’n step on my name, you c’n try ’n’ get me beat\\n\\nWhen I leave New York, I’ll be standin’ on my feet\\n\\nAnd it’s hard times in the city\\n\\nLivin’ down in New York town'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate lyrics into one string\n",
        "text = (\n",
        "  ''.join(\n",
        "    df['lyrics']\n",
        "    .str.replace('\\n \\n \\n \\n ', '\\n\\n\\n')\n",
        "    .str.replace('\\n \\n \\n ', '\\n\\n\\n')\n",
        "    .str.replace('\\n \\n ', '\\n\\n')\n",
        "    .str.replace('\\n ', '\\n')\n",
        "    .str.replace('\\n\\n\\n', '\\n\\n')\n",
        "    .str.replace('\\n\\n', '\\n')\n",
        "  )\n",
        ")"
      ],
      "metadata": {
        "id": "PYxCG5ircOle"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "n = len(text)\n",
        "train_data = text[:int(n*0.9)]\n",
        "val_data = text[int(n*0.9):]\n",
        "\n",
        "n, len(train_data), len(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL16_ObbcOek",
        "outputId": "bbf2ca16-4c3e-4d70-b7a0-7ce7db506b1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(523786, 471407, 52379)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Encode\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "\n",
        "# Dtype\n",
        "train_data = np.array(train_ids, dtype=np.uint16)\n",
        "val_data = np.array(val_ids, dtype=np.uint16)\n",
        "\n",
        "print(f\"train has {len(train_data):,} tokens\")\n",
        "print(f\"val has {len(val_data):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3wplekLcq3N",
        "outputId": "d5e24123-cdf9-47d3-9763-c3214d9f23b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 136,916 tokens\n",
            "val has 14,718 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sorted(list(set(train_ids + val_ids)))\n",
        "vocab_size = len(tokens)\n",
        "\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6jq-F8Yr0cC",
        "outputId": "689c4e4f-f487-449b-de53-5b3a5af586e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8861"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9YlypBpwRsI",
        "outputId": "11988af5-6f6f-48c6-ab72-bc33f5b08c33"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16773,   345, 17308,   290,   345, 28527,    11,   257,    12,\n",
              "        4868], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max token number\n",
        "max(max(train_data),max(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvjdd-9EzgSi",
        "outputId": "9a37325d-af2b-48da-920b-800983cd7b97"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50255"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "mqP7iydhQ289"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Hyperparamters - gpu\n",
        "# batch_size = 64      # number of independent sequences to process in parallel\n",
        "# block_size = 256     # context length\n",
        "# max_iters = 5_000    # training iterations\n",
        "# eval_interval = 500  # how often to evaluate the loss on train and val sets\n",
        "# eval_iters = 200     # how many iterations to average loss over when evaluating\n",
        "# learning_rate = 3e-4 # learning rate param\n",
        "# n_embed = 384         # embedding size\n",
        "# n_layer = 6          # number of transformer blocks\n",
        "# dropout = 0.2        # dropout rate\n",
        "# n_head = 6           # number of heads\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "p4bE39y42yT1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Hyperparamters - cpu\n",
        "# batch_size = 32      # number of independent sequences to process in parallel\n",
        "# block_size = 16       # context length\n",
        "# max_iters = 5_000    # training iterations\n",
        "# eval_interval = 100  # how often to evaluate the loss on train and val sets\n",
        "# eval_iters = 200     # how many iterations to average loss over when evaluating\n",
        "# learning_rate = 1e-3 # learning rate param\n",
        "# n_embed = 128         # embedding size\n",
        "# n_layer = 4          # number of transformer blocks\n",
        "# dropout = 0.0        # dropout rate\n",
        "# n_head = 4           # number of heads\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "SRbscvWO3ecZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RS"
      ],
      "metadata": {
        "id": "Xw3-vxckBGLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamters - gpu v1 - char format\n",
        "\n",
        "out_dir = '/content/out-char'\n",
        "eval_interval = 250  # how often to evaluate the loss on train and val sets\n",
        "eval_iters = 200     # how many iterations to average loss over when evaluating\n",
        "log_interval = 10    # dont print too often\n",
        "\n",
        "# we expct to overfit: save when val improves\n",
        "always_save_checkpoint = False\n",
        "\n",
        "# logger\n",
        "wandb_log = False\n",
        "wandb_project = 'dylan-char'\n",
        "wandb_run_name = 'mini-gpt'\n",
        "\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64      # number of independent sequences to process in parallel\n",
        "block_size = 256     # context length\n",
        "\n",
        "\n",
        "# baby gpt-model\n",
        "learning_rate = 1e-4\n",
        "max_iters = 5_000    # training iterations\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-4        # lr / 10\n",
        "beta2 = 0.99         # make bigger because number of tokens per iters is small\n",
        "warm_up_iters = 100\n",
        "\n",
        "n_embed = 384        # embedding size\n",
        "n_layer = 6          # number of transformer blocks\n",
        "n_head = 6           # number of heads of each transformer block\n",
        "dropout = 0.2        # dropout rate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Hlm6WzLHcfju"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch data"
      ],
      "metadata": {
        "id": "gJxco2CTOZ4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    \"\"\"Generate a small random batch of data of inputs 'x'\n",
        "    and targets 'y'\n",
        "    \"\"\"\n",
        "    # get random index\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(\n",
        "        low=0,\n",
        "        high=len(data) - block_size,\n",
        "        size=(batch_size,))\n",
        "\n",
        "    # get the context for each index, and stack it into\n",
        "    # rows.\n",
        "    x = torch.stack([torch.from_numpy((data[i : i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1 : i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "\n",
        "\n",
        "    #if device == 'cuda':\n",
        "      # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "      # x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "\n",
        "    #else:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "_TGc4VrxOb1W"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = get_batch(split='train')"
      ],
      "metadata": {
        "id": "91lIgURhRWVz"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0DExn6xvYwI",
        "outputId": "939b9c54-6e99-462b-80ba-81ea9f30081a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0], y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHKBezeZRZ5u",
        "outputId": "f3be0fa3-bec4-4bdb-b514-8d86aef4fa6a"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0, 32, 64,  2, 58, 55, 51, 72, 55, 64,  2, 65, 68,  2, 59, 64,  2, 58,\n",
              "         55, 62, 62, 23,  0,  0, 24, 68, 55,  2, 75, 65, 71,  2, 68, 55, 51, 54,\n",
              "         75,  8,  2, 51, 68, 55,  2, 75, 65, 71,  2, 68, 55, 51, 54, 75, 23,  0,\n",
              "          0, 31, 51, 72, 55,  2, 75, 65, 71,  2, 57, 65, 70,  2, 69, 65, 63, 55,\n",
              "          2, 71, 64, 56, 59, 64, 59, 69, 58, 55, 54,  2, 52, 71, 69, 59, 64, 55,\n",
              "         69, 69, 23,  0, 32, 69,  2, 70, 58, 55, 68, 55,  2, 69, 65, 63, 55, 70,\n",
              "         58, 59, 64, 57,  2, 58, 65, 62, 54, 59, 64, 57,  2, 75, 65, 71,  2, 52,\n",
              "         51, 53, 61, 23,  0, 24, 68, 55,  2, 75, 65, 71,  2, 70, 58, 59, 64, 61,\n",
              "         59, 64, 57,  2, 56, 65, 68,  2, 75, 65, 71, 68, 69, 55, 62, 56,  0, 38,\n",
              "         68,  2, 51, 68, 55,  2, 75, 65, 71,  2, 56, 65, 62, 62, 65, 73, 59, 64,\n",
              "         57,  2, 70, 58, 55,  2, 66, 51, 53, 61, 23,  0,  0, 24, 68, 55,  2, 75,\n",
              "         65, 71,  2, 68, 55, 51, 54, 75,  8,  2, 58, 65, 66, 55,  2, 75, 65, 71,\n",
              "         86, 68, 55,  2, 68, 55, 51, 54, 75,  0, 24, 68, 55,  2, 75, 65, 71,  2,\n",
              "         68, 55, 51, 54, 75, 23,  0,  0, 24, 68, 55,  2, 75, 65, 71,  2, 68, 55,\n",
              "         51, 54, 75,  2], device='cuda:0'),\n",
              " tensor([32, 64,  2, 58, 55, 51, 72, 55, 64,  2, 65, 68,  2, 59, 64,  2, 58, 55,\n",
              "         62, 62, 23,  0,  0, 24, 68, 55,  2, 75, 65, 71,  2, 68, 55, 51, 54, 75,\n",
              "          8,  2, 51, 68, 55,  2, 75, 65, 71,  2, 68, 55, 51, 54, 75, 23,  0,  0,\n",
              "         31, 51, 72, 55,  2, 75, 65, 71,  2, 57, 65, 70,  2, 69, 65, 63, 55,  2,\n",
              "         71, 64, 56, 59, 64, 59, 69, 58, 55, 54,  2, 52, 71, 69, 59, 64, 55, 69,\n",
              "         69, 23,  0, 32, 69,  2, 70, 58, 55, 68, 55,  2, 69, 65, 63, 55, 70, 58,\n",
              "         59, 64, 57,  2, 58, 65, 62, 54, 59, 64, 57,  2, 75, 65, 71,  2, 52, 51,\n",
              "         53, 61, 23,  0, 24, 68, 55,  2, 75, 65, 71,  2, 70, 58, 59, 64, 61, 59,\n",
              "         64, 57,  2, 56, 65, 68,  2, 75, 65, 71, 68, 69, 55, 62, 56,  0, 38, 68,\n",
              "          2, 51, 68, 55,  2, 75, 65, 71,  2, 56, 65, 62, 62, 65, 73, 59, 64, 57,\n",
              "          2, 70, 58, 55,  2, 66, 51, 53, 61, 23,  0,  0, 24, 68, 55,  2, 75, 65,\n",
              "         71,  2, 68, 55, 51, 54, 75,  8,  2, 58, 65, 66, 55,  2, 75, 65, 71, 86,\n",
              "         68, 55,  2, 68, 55, 51, 54, 75,  0, 24, 68, 55,  2, 75, 65, 71,  2, 68,\n",
              "         55, 51, 54, 75, 23,  0,  0, 24, 68, 55,  2, 75, 65, 71,  2, 68, 55, 51,\n",
              "         54, 75,  2, 56], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimate loss"
      ],
      "metadata": {
        "id": "lL3-7YTETIBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"Averages out the loss over multiple batches, in order\n",
        "    to get a better estimate of the loss.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "\n",
        "    # set model to evaluation phase\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    # set model to training phase\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "Peq7wepqRzeo"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer `Head`"
      ],
      "metadata": {
        "id": "0-8KqDj7TXH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define `Self Attention` model. The idea behind `self-attention` is to gather information from the past, in a data-dependant way."
      ],
      "metadata": {
        "id": "SzeFNLqUWnOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example,\n",
        "\n",
        "$X_{4x8x32} * X_{32x16} = X_{4x8x16}$\n",
        "\n",
        "where,\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "context_length = 8\n",
        "\n",
        "embedding_dimension = 32\n",
        "\n",
        "head_size = 16"
      ],
      "metadata": {
        "id": "HKnTqxL0crdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\"One head of self-attention.\n",
        "  The idea is that each token has a:\n",
        "    - key\n",
        "    - query\n",
        "    - value\n",
        "  Then, for each token, the key queries every other token, and \"resonates\"\n",
        "  with those where the link is stronger. These relationships are stored in\n",
        "  a \"weights\" vector, which is applied over the value of each token, to weight\n",
        "  each token in the past context.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # Create independent and parallel key and query for each token\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer(\n",
        "        'tril',\n",
        "        torch.tril(torch.ones(block_size, block_size))\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)   # (B, T, C) @ (C, head_size) -> (B, T, head_size)\n",
        "    q = self.query(x) # (B, T, C) @ (C, head_size) -> (B, T, head_size)\n",
        "\n",
        "    # Compute attention scores, i.e. affinities\n",
        "    wei = q @ k.transpose(-1, -2) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    # Perform the weighted aggregation of the values\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "ic20NvmvTKYR"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MulitHeadAttention(nn.Module):\n",
        "  \"\"\"Multiple heads of self-attention in parallel.\n",
        "  Note: head_size * n_heads = n_embed\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.heads = nn.ModuleList(\n",
        "        [Head(head_size) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Concatenate the heads over the channel dimension, i.e. horizontally,\n",
        "    # one besides the other, column-wise\n",
        "    out = torch.cat(\n",
        "        [h(x) for h in self.heads],\n",
        "        dim=-1\n",
        "    ) # (B, T, head_size * num_heads)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "51OtHTR3WwZB"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\"A simple linear layer followed by a non-linearity\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4 * n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embed, n_embed),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "bQVCnq46W4OY"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  \"\"\"Transformer block: communication followed by computation\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MulitHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "HpBYcxikXEQn"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DylanGPT Model"
      ],
      "metadata": {
        "id": "jts1n5x4t8Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DylanGPT(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # Embeddings\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.postition_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "\n",
        "    # Transformer Blocks\n",
        "    self.blocks = nn.Sequential(*[\n",
        "        TransformerBlock(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "    # Final layer norm\n",
        "    self.ln_f = nn.LayerNorm(n_embed)\n",
        "\n",
        "    # LLM head\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    B, T = idx.shape\n",
        "    token_embed = self.token_embedding_table(idx)\n",
        "    pos_embed = self.postition_embedding_table(torch.arange(T, device=device))\n",
        "\n",
        "    x = token_embed + pos_embed\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # F.cross_entropy requires a (B, C, T) shape\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      logits, loss = self(idx_cond)\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "MDWjzx4-0ImD"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "2DcosgCH2tx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# vocab size\n",
        "#vocab_size = 50257\n",
        "\n",
        "# model\n",
        "model = DylanGPT(vocab_size=vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# loop\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlUrIsaw0Ijx",
        "outputId": "ff1182c5-afb2-4fd5-e147-8570aa92d72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6613, val loss 4.6762\n",
            "step 250: train loss 2.4087, val loss 2.4487\n",
            "step 500: train loss 2.3588, val loss 2.4106\n",
            "step 750: train loss 2.2637, val loss 2.3228\n",
            "step 1000: train loss 2.0438, val loss 2.1289\n",
            "step 1250: train loss 1.9045, val loss 2.0089\n",
            "step 1500: train loss 1.7985, val loss 1.9187\n",
            "step 1750: train loss 1.7158, val loss 1.8511\n",
            "step 2000: train loss 1.6443, val loss 1.7969\n",
            "step 2250: train loss 1.5958, val loss 1.7597\n",
            "step 2500: train loss 1.5415, val loss 1.7264\n",
            "step 2750: train loss 1.4981, val loss 1.6976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To do:\n",
        "- add plots for loss (train/val) and others\n",
        "- find a good lr\n",
        "- optimize initialization\n",
        "- optimize learning rate"
      ],
      "metadata": {
        "id": "byhcw7RIUtLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate from model\n",
        "context = torch.zeros((1,1),dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_-zsRms0IhI",
        "outputId": "7639e430-40d6-458e-a12a-3b98cc2f6ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Yes, my botte sound so many offfered\n",
            "gainst to past the conview—I went years on xth\n",
            "But I could not steal\n",
            "Yes, it’s easy I tought on the others\n",
            "Go on the author, Home times warth in shore\n",
            "Poor boy, let us it love\n",
            "Of sister, what they’ll be?\n",
            "Can you make it is You Do?I ran For\n",
            "If I’m leavin’ hard low\n",
            "You just think they see me\n",
            "But with me just someone love\n",
            "If you see just might like just memories\n",
            "Hang you sit\n",
            "If I see from than a cost\n",
            "It everything’s that beer\n",
            "So one time at all\n",
            "Things so work ev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add temperature"
      ],
      "metadata": {
        "id": "qslkvGByUW9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = 0.7 # (0,1])\n",
        "\n",
        "idx = torch.zeros((1,1),dtype=torch.long, device=device)\n",
        "for _ in range(500):\n",
        "    idx_cond = idx[:, -block_size:]\n",
        "\n",
        "    logits, loss = m(idx_cond)\n",
        "\n",
        "    logits = logits[:, -1, :] * 1/T\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "print(decode(idx[0].detach().cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BMoe38t6GP3",
        "outputId": "0c461a6f-6271-4890-9fec-cb69d8ed87d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "And it’s too late to say\n",
            "I mustand it\n",
            "But it looks like you feel\n",
            "They from death it the country fail\n",
            "And caused the locusts\n",
            "A can heavent for you choser they hear their beat\n",
            "If you can tell Henry Louisiecon you and deny\n",
            "All comered then the door think they could not say your room?\n",
            "\n",
            "She said, “Darlin’ have there where your death\n",
            "Don’t you look at me, I’m through\n",
            "I can still house the world of meathing\n",
            "You’ve heard the burns out\n",
            "You picked me out when the way it isn’t here\n",
            "How long much longer cra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = 0.6 # (0,1])\n",
        "\n",
        "idx = torch.zeros((1,1),dtype=torch.long, device=device)\n",
        "for _ in range(500):\n",
        "    idx_cond = idx[:, -block_size:]\n",
        "\n",
        "    logits, loss = m(idx_cond)\n",
        "\n",
        "    logits = logits[:, -1, :] * 1/T\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "print(decode(idx[0].detach().cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r10aPiNW6yH6",
        "outputId": "683b61e8-0180-4fec-c6af-cd89a8e4208b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hast one day has not stay to take\n",
            "Then the seven the truth, they brown in the artend\n",
            "But it was just like to see yourself\n",
            "And you never done it all that we’ve had enough\n",
            "But the country for look away\n",
            "\n",
            "I know the sun return the sound\n",
            "But the time we were so coming to stop, we kind\n",
            "The neighborhought and we keep who would be not fency\n",
            "The crampion of the platfor the gutter\n",
            "“Come in, on, some come back egging for me,”\n",
            "“It’s gonna stake you be right here”\n",
            "Did someone who knows you can take your \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kqpI5SgXc8lZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}